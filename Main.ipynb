{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b99a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized images done\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "input_base_folder = 'Dataset Proper/Test'\n",
    "output_base_folder = 'New Dataset_proper_resized/test'\n",
    "\n",
    "# List of class folders\n",
    "class_folders = os.listdir(input_base_folder)\n",
    "\n",
    "# Iterate through class folders\n",
    "for class_folder in class_folders:\n",
    "    input_folder = os.path.join(input_base_folder, class_folder)\n",
    "    output_folder = os.path.join(output_base_folder, class_folder)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Resize images and save them to the output folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Load the image using PIL\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Resize the image to your desired size (e.g., 224x224)\n",
    "        target_size = (224, 224)\n",
    "        image = image.resize(target_size)\n",
    "\n",
    "        # Save the resized image to the output folder\n",
    "        save_path = os.path.join(output_folder, filename)\n",
    "        image.save(save_path)\n",
    "\n",
    "print(\"Resized images done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2551a519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise removal and PNG conversion completed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Path to the directory containing the Herlev dataset\n",
    "dataset_dir = 'New Dataset_proper_resized/test'\n",
    "\n",
    "# Detect classes automatically\n",
    "classes = [class_name for class_name in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, class_name))]\n",
    "\n",
    "# Define the NLM filter function\n",
    "def nlm_filter(image):\n",
    "    filtered_image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)  # Adjust parameters as needed\n",
    "    return filtered_image\n",
    "\n",
    "# Iterate through each class\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    output_dir = os.path.join('New NLM Filter/test', class_name, f'{class_name}_filtered_nlm')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # List all image files in the class directory\n",
    "    image_files = [os.path.join(class_dir, filename) for filename in os.listdir(class_dir) if filename.lower().endswith('.bmp')]\n",
    "\n",
    "    # Apply NLM filtering to each image, convert to PNG, and save the filtered images\n",
    "    for image_file in image_files:\n",
    "        # Load the image using OpenCV\n",
    "        image = cv2.imread(image_file)\n",
    "\n",
    "        # Convert the image to RGB (OpenCV reads images in BGR format)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Apply NLM filtering\n",
    "        filtered_image = nlm_filter(image_rgb)\n",
    "\n",
    "        # Get the file name without extension\n",
    "        filename = os.path.splitext(os.path.basename(image_file))[0]\n",
    "\n",
    "        # Save the filtered image as PNG format\n",
    "        output_file = os.path.join(output_dir, f'{filename}_filtered.png')\n",
    "        cv2.imwrite(output_file, filtered_image)\n",
    "\n",
    "print(\"Noise removal and PNG conversion completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aaeb989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented images done\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Define the base input and output folders\n",
    "base_input_folder = 'New NLM Filter/test'\n",
    "base_output_folder = 'New NLM Augmentation/test'\n",
    "\n",
    "# Automatically detect class folders\n",
    "class_folders = [folder for folder in os.listdir(base_input_folder) if os.path.isdir(os.path.join(base_input_folder, folder))]\n",
    "\n",
    "# Create an ImageDataGenerator and specify the augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    rescale=1./255  # Normalize pixel values\n",
    ")\n",
    "\n",
    "# Loop through each class folder\n",
    "for class_folder in class_folders:\n",
    "    input_folder = os.path.join(base_input_folder, class_folder)\n",
    "    output_folder = os.path.join(base_output_folder, class_folder)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Generate augmented images and save them to the output folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        image_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Load the image using Keras' load_img function\n",
    "        image = load_img(image_path)\n",
    "\n",
    "        # Convert the image to a NumPy array\n",
    "        image_array = img_to_array(image)\n",
    "\n",
    "        # Reshape the image array to match the expected input shape of the generator\n",
    "        image_array = image_array.reshape((1,) + image_array.shape)\n",
    "\n",
    "        # Generate augmented images using the datagen.flow() method\n",
    "        augmented_images = datagen.flow(\n",
    "            image_array,\n",
    "            batch_size=16,\n",
    "            save_to_dir=output_folder,\n",
    "            save_prefix='augmented',\n",
    "            save_format='png'\n",
    "        )\n",
    "\n",
    "        # Generate and save the augmented images\n",
    "        num_augmented_images = 6\n",
    "        for i, augmented_image in enumerate(augmented_images):\n",
    "            if i >= num_augmented_images:\n",
    "                break\n",
    "\n",
    "            augmented_image_pil = Image.fromarray((augmented_image[0] * 255).astype('uint8'))  # Ensure correct range\n",
    "\n",
    "            save_filename = f'{filename.split(\".\")[0]}_{i}.png'\n",
    "            save_path = os.path.join(output_folder, save_filename)\n",
    "            augmented_image_pil.save(save_path)\n",
    "\n",
    "print(\"Augmented images done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b2a9d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the directories for train and test datasets\n",
    "train_dir = 'New NLM Augmentation/train'  # Replace with your train dataset directory\n",
    "test_dir = 'New NLM Augmentation/test'    # Replace with your test dataset directory\n",
    "output_dir = 'NEW_DATASET_SIZE'  # Replace with the directory where you want to store the merged dataset\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List the classes in the train dataset (assuming each subdirectory corresponds to a class)\n",
    "classes = os.listdir(train_dir)\n",
    "\n",
    "# Merge train and test datasets by copying files to the output directory\n",
    "for class_name in classes:\n",
    "    train_class_dir = os.path.join(train_dir, class_name)\n",
    "    test_class_dir = os.path.join(test_dir, class_name)\n",
    "    output_class_dir = os.path.join(output_dir, class_name)\n",
    "    \n",
    "    # Create the class directory in the merged dataset if it doesn't exist\n",
    "    os.makedirs(output_class_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy images from the train dataset\n",
    "    train_files = os.listdir(train_class_dir)\n",
    "    for file in train_files:\n",
    "        src_path = os.path.join(train_class_dir, file)\n",
    "        dst_path = os.path.join(output_class_dir, file)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    # Copy images from the test dataset\n",
    "    test_files = os.listdir(test_class_dir)\n",
    "    for file in test_files:\n",
    "        src_path = os.path.join(test_class_dir, file)\n",
    "        dst_path = os.path.join(output_class_dir, file)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"Merging complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f774dd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herlev dataset split into train, validation, and test sets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Set your data directory\n",
    "data_dir = 'NEW_DATASET_SIZE'  # Replace with the path to your Herlev dataset\n",
    "\n",
    "# Define the directory names for the splits\n",
    "train_dir = 'NEW_DATASET/train'\n",
    "val_dir = 'NEW_DATASET/validation'\n",
    "test_dir = 'NEW_DATASET/test'\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Define the ratio for splitting (e.g., 70% train, 15% validation, 15% test)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.18\n",
    "test_ratio = 0.12\n",
    "\n",
    "# Loop through each class in the dataset\n",
    "for class_name in os.listdir(data_dir):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    if os.path.isdir(class_dir):\n",
    "        # List all the files in the class directory\n",
    "        files = os.listdir(class_dir)\n",
    "        random.shuffle(files)  # Shuffle the files\n",
    "        \n",
    "        num_files = len(files)\n",
    "        \n",
    "        # Calculate the number of samples for each split\n",
    "        num_train = int(train_ratio * num_files)\n",
    "        num_val = int(val_ratio * num_files)\n",
    "        num_test = num_files - num_train - num_val\n",
    "        \n",
    "        # Create subdirectories for each split\n",
    "        class_train_dir = os.path.join(train_dir, class_name)\n",
    "        class_val_dir = os.path.join(val_dir, class_name)\n",
    "        class_test_dir = os.path.join(test_dir, class_name)\n",
    "        \n",
    "        os.makedirs(class_train_dir, exist_ok=True)\n",
    "        os.makedirs(class_val_dir, exist_ok=True)\n",
    "        os.makedirs(class_test_dir, exist_ok=True)\n",
    "        \n",
    "        # Copy files to respective splits\n",
    "        for i, file in enumerate(files):\n",
    "            src_path = os.path.join(class_dir, file)\n",
    "            if i < num_train:\n",
    "                dst_path = os.path.join(class_train_dir, file)\n",
    "            elif i < num_train + num_val:\n",
    "                dst_path = os.path.join(class_val_dir, file)\n",
    "            else:\n",
    "                dst_path = os.path.join(class_test_dir, file)\n",
    "            \n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "print(\"Herlev dataset split into train, validation, and test sets.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
